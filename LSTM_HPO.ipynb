{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJ/e4s2SgplQ+l/UngWx+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FVLegion/AI-Studio-ClearML/blob/main/LSTM_HPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Load and Preprocess the Dataset**\n",
        "\n",
        "First, we need to load your dataset. If you have your own dataset, you'll need to load it accordingly (e.g., from a CSV, text files, etc.). For this example, we'll use the IMDb dataset."
      ],
      "metadata": {
        "id": "ncVqfm9F6GoN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLvbYeLN5nRR",
        "outputId": "1ec7d8b2-0f95-4120-e612-8609fde3d91d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded and preprocessed.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the dataset\n",
        "max_features = 10000  # Number of words to consider as features\n",
        "maxlen = 500  # Cut texts after this many words (among top max_features most common words)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "print(\"Dataset loaded and preprocessed.\")"
      ]
    },
    {
      "source": [
        "# Get the word index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Reverse the word index to map integer indices to words\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# Decode a sample review\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in text])\n",
        "\n",
        "# Number of samples to display\n",
        "num_samples_to_display = 5 # You can change this number\n",
        "\n",
        "print(f\"Displaying the first {num_samples_to_display} sample reviews:\\n\")\n",
        "\n",
        "for i in range(num_samples_to_display):\n",
        "    decoded_review = decode_review(x_train[i])\n",
        "    sample_label = y_train[i]\n",
        "\n",
        "    print(f\"Sample Review (Index {i}):\")\n",
        "    print(decoded_review)\n",
        "    print(f\"Sample Label: {sample_label} {'(Positive)' if sample_label == 1 else '(Negative)'}\")\n",
        "    print(\"-\" * 50) # Print a separator for clarity"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7TXI6DjAMEI",
        "outputId": "396960d7-a77e-410f-f4f4-90b2e4b33412"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying the first 5 sample reviews:\n",
            "\n",
            "Sample Review (Index 0):\n",
            "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "Sample Label: 1 (Positive)\n",
            "--------------------------------------------------\n",
            "Sample Review (Index 1):\n",
            "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal ? the hair is big lots of boobs ? men wear those cut ? shirts that show off their ? sickening that men actually wore them and the music is just ? trash that plays over and over again in almost every scene there is trashy music boobs and ? taking away bodies and the gym still doesn't close for ? all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n",
            "Sample Label: 0 (Negative)\n",
            "--------------------------------------------------\n",
            "Sample Review (Index 2):\n",
            "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had ? working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how ? this is to watch save yourself an hour a bit of your life\n",
            "Sample Label: 0 (Negative)\n",
            "--------------------------------------------------\n",
            "Sample Review (Index 3):\n",
            "events on the ? heath a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in bars the length and ? of scotland as i discussed it with a friend one night in ? a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional ? fact and fiction blend with ? role models warning stories ? magic and mystery br br my name is ? like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that stretches back through generations it produces stories within stories stories that evoke the ? wonder of scotland its rugged mountains ? in ? the stuff of legend yet ? is ? in reality this is what gives it its special charm it has a rough beauty and authenticity ? with some of the finest ? singing you will ever hear br br ? ? visits his grandfather in hospital shortly before his death he burns with frustration part of him ? to be in the twenty first century to hang out in ? but he is raised on the western ? among a ? speaking community br br yet there is a deeper conflict within him he ? to know the truth the truth behind his ? ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last ? journey to the ? of one of ? most ? mountains can the truth be told or is it all in stories br br in this story about stories we ? bloody battles ? lovers the ? of old and the sometimes more ? ? of accepted truth in doing so we each connect with ? as he lives the story of his own life br br ? the ? ? is probably the most honest ? and genuinely beautiful film of scotland ever made like ? i got slightly annoyed with the ? of hanging stories on more stories but also like ? i ? this once i saw the ? picture ' forget the box office ? of braveheart and its like you might even ? the ? famous ? of the wicker man to see a film that is true to scotland this one is probably unique if you maybe ? on it deeply enough you might even re ? the power of storytelling and the age old question of whether there are some truths that cannot be told but only experienced\n",
            "Sample Label: 1 (Positive)\n",
            "--------------------------------------------------\n",
            "Sample Review (Index 4):\n",
            "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon bashing my head in i stuck that damn movie in the ? and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and ruining a whole day of my life\n",
            "Sample Label: 0 (Negative)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Build the BiLSTM Model**\n",
        "\n",
        "Now, let's define a function to build your BiLSTM model. This will be useful for hyperparameter tuning later, as you can easily change the model architecture based on the hyperparameters."
      ],
      "metadata": {
        "id": "yp2n1lGq6PW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "def build_bilstm_model(embedding_dim=128, lstm_units=128, dense_units=64, dropout_rate=0.5):\n",
        "    model = Sequential([\n",
        "        Embedding(max_features, embedding_dim, input_length=maxlen),\n",
        "        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
        "        Bidirectional(LSTM(lstm_units)),\n",
        "        Dense(dense_units, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "ydakUmB06TQe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Train the BiLSTM Model**\n",
        "\n",
        "We can now train a baseline model with some default hyperparameters."
      ],
      "metadata": {
        "id": "TKSVg1ib6ZBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the baseline model\n",
        "model = build_bilstm_model()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "print(\"\\nBaseline model training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCYPXypW6XKd",
        "outputId": "c858e83b-6bd9-41cf-b9b7-5501a42441e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 84ms/step - accuracy: 0.6841 - loss: 0.5716 - val_accuracy: 0.7846 - val_loss: 0.4277\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.8597 - loss: 0.3438 - val_accuracy: 0.7386 - val_loss: 0.4917\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.8885 - loss: 0.2849 - val_accuracy: 0.8398 - val_loss: 0.3868\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - accuracy: 0.9344 - loss: 0.1828 - val_accuracy: 0.8668 - val_loss: 0.3472\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.9581 - loss: 0.1212 - val_accuracy: 0.8588 - val_loss: 0.4380\n",
            "\n",
            "Baseline model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Hyperparameter Tuning**\n",
        "\n",
        "For hyperparameter tuning, we can use libraries like Keras Tuner or Hyperopt. Here, I'll show you a simple example using a grid search approach with scikit-learn's GridSearchCV by wrapping your Keras model with KerasClassifier.\n",
        "\n",
        "First, lets install Keras Tuner:"
      ],
      "metadata": {
        "id": "5IGbFnKW6f8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install keras-tuner==1.0.2\n",
        "!pip install keras-tuner\n",
        "# Add a step to verify the installation\n",
        "!pip show keras-tuner\n",
        "\n",
        "import keras_tuner as kt\n",
        "\n",
        "def build_model_for_tuning(hp):\n",
        "    embedding_dim = hp.Int('embedding_dim', min_value=64, max_value=256, step=32)\n",
        "    lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=32)\n",
        "    dense_units = hp.Int('dense_units', min_value=32, max_value=128, step=32)\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1)\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(max_features, embedding_dim, input_length=maxlen),\n",
        "        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
        "        Bidirectional(LSTM(lstm_units)),\n",
        "        Dense(dense_units, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model_for_tuning,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,  # Number of hyperparameter combinations to try\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='bilstm_tuning')\n",
        "\n",
        "# Perform the search\n",
        "tuner.search(x_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The optimal hyperparameters are:\n",
        "Embedding Dimension: {best_hps.get('embedding_dim')}\n",
        "LSTM Units: {best_hps.get('lstm_units')}\n",
        "Dense Units: {best_hps.get('dense_units')}\n",
        "Dropout Rate: {best_hps.get('dropout_rate')}\n",
        "\"\"\")\n",
        "\n",
        "# Build and train the final model with the best hyperparameters\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "print(\"\\nTraining the final model with optimal hyperparameters.\")\n",
        "# You can train this model for more epochs if needed\n",
        "# history_tuned = best_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "8d9rlL826pWo",
        "outputId": "cebe6e14-a0c7-48a3-b864-e38662ed469a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.11/dist-packages (1.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (0.9.0)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.1.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (1.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->keras-tuner) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->keras-tuner) (3.6.0)\n",
            "Name: keras-tuner\n",
            "Version: 1.0.2\n",
            "Summary: Hypertuner for Keras\n",
            "Home-page: https://github.com/keras-team/keras-tuner\n",
            "Author: The Keras Tuner authors\n",
            "Author-email: kerastuner@google.com\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: colorama, future, numpy, packaging, requests, scikit-learn, scipy, tabulate, terminaltables, tqdm\n",
            "Required-by: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras_tuner'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3ad75e0366c2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip show keras-tuner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tuner\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model_for_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_tuner'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "1.  **Data Loading and Preprocessing:** We load the IMDb dataset and use `pad_sequences` to ensure all input sequences have the same length, which is required for batching in neural networks.\n",
        "2.  **Model Building Function:** We define a function `build_bilstm_model` that creates the BiLSTM architecture. This function takes hyperparameters as arguments, making it easy to build different model variations during tuning.\n",
        "3.  **Baseline Training:** We train a basic model to get a sense of performance before tuning.\n",
        "4.  **Hyperparameter Tuning with Keras Tuner:**\n",
        "    *   We define `build_model_for_tuning` which is similar to `build_bilstm_model` but uses `hp` (HyperParameters) to define the search space for each hyperparameter.\n",
        "    *   `kt.RandomSearch` is initialized to search for the best hyperparameters. We set the objective to maximize validation accuracy.\n",
        "    *   `tuner.search` runs the hyperparameter search, training different models with different combinations of hyperparameters.\n",
        "    *   `tuner.get_best_hyperparameters` retrieves the best set of hyperparameters found.\n",
        "    *   `tuner.get_best_models` retrieves the best-performing model.\n",
        "\n",
        "**Important Considerations:**\n",
        "\n",
        "*   **Dataset:** Replace the IMDb data loading with your own dataset loading and preprocessing steps.\n",
        "*   **Hyperparameter Space:** The hyperparameter ranges and steps in `build_model_for_tuning` are examples. You should adjust them based on your specific problem and dataset.\n",
        "*   **Tuning Algorithm:** Keras Tuner offers other tuning algorithms like `BayesianOptimization`. You can explore these for potentially better results.\n",
        "*   **Epochs and Batch Size:** The number of epochs and batch size used during tuning and final training should be chosen carefully.\n",
        "*   **Validation Set:** It's crucial to use a validation set during tuning to avoid overfitting to the training data.\n",
        "*   **Evaluation:** After tuning, evaluate your best model on the test set to get a realistic estimate of its performance."
      ],
      "metadata": {
        "id": "upSiP0kM6-hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Optuna\n",
        "!pip install optuna tensorflow\n",
        "\n",
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "import optuna\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset (re-including this for clarity, assuming the kernel was restarted)\n",
        "max_features = 10000  # Number of words to consider as features\n",
        "maxlen = 500  # Cut texts after this many words (among top max_features most common words)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "print(\"Dataset loaded and preprocessed.\")\n",
        "\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    embedding_dim = trial.suggest_int('embedding_dim', 64, 256, step=32)\n",
        "    lstm_units = trial.suggest_int('lstm_units', 64, 256, step=32)\n",
        "    dense_units = trial.suggest_int('dense_units', 32, 128, step=32)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.3, 0.7, step=0.1)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "\n",
        "    # Build the model with suggested hyperparameters\n",
        "    model = Sequential([\n",
        "        Embedding(max_features, embedding_dim, input_length=maxlen),\n",
        "        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
        "        Bidirectional(LSTM(lstm_units)),\n",
        "        Dense(dense_units, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model with a suggested learning rate\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    # Use validation_split for evaluation during tuning\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        epochs=5, # Use a reasonable number of epochs for tuning\n",
        "                        batch_size=32,\n",
        "                        validation_split=0.2,\n",
        "                        verbose=0) # Set verbose to 0 to reduce output during tuning\n",
        "\n",
        "    # Return the validation accuracy as the objective value to minimize (or maximize by returning negative)\n",
        "    # Optuna by default minimizes, so we return negative validation accuracy to maximize it\n",
        "    return history.history['val_accuracy'][-1] # Return the accuracy from the last epoch\n",
        "\n",
        "\n",
        "# Create a study and optimize\n",
        "# Specify direction='maximize' to maximize the validation accuracy\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10) # Run 10 trials\n",
        "\n",
        "# Print the best hyperparameters and objective value\n",
        "print(\"\\nOptuna study complete.\")\n",
        "print(\"Best hyperparameters: \", study.best_params)\n",
        "print(\"Best validation accuracy: \", study.best_value)\n",
        "\n",
        "# You can then build and train the final model with the best hyperparameters found by Optuna\n",
        "best_hps = study.best_params\n",
        "\n",
        "print(\"\\nTraining the final model with optimal hyperparameters.\")\n",
        "\n",
        "# Build the final model with the best hyperparameters\n",
        "final_model = Sequential([\n",
        "    Embedding(max_features, best_hps['embedding_dim'], input_length=maxlen),\n",
        "    Bidirectional(LSTM(best_hps['lstm_units'], return_sequences=True)),\n",
        "    Bidirectional(LSTM(best_hps['lstm_units'])),\n",
        "    Dense(best_hps['dense_units'], activation='relu'),\n",
        "    tf.keras.layers.Dropout(best_hps['dropout_rate']),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the final model with the best learning rate if it was tuned\n",
        "final_optimizer = tf.keras.optimizers.Adam(learning_rate=best_hps.get('learning_rate', 0.001)) # Use default if not tuned\n",
        "final_model.compile(optimizer=final_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the final model (you might want to train for more epochs here)\n",
        "# history_tuned = final_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "print(\"\\nFinal model with optimal hyperparameters built and compiled.\")\n",
        "# Note: The final model is built but not trained in this block.\n",
        "# You would typically train it in a subsequent cell with the full training data\n",
        "# or with a different number of epochs."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgyBU6O36-9A",
        "outputId": "8e91662f-5be8-409a-f834-81d8f01fdac9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-20 14:05:22,547] A new study created in memory with name: no-name-5a89e5c2-673d-433f-ae26-d4cffc93fa33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded and preprocessed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n",
            "[I 2025-05-20 14:11:51,378] Trial 0 finished with value: 0.8741999864578247 and parameters: {'embedding_dim': 64, 'lstm_units': 160, 'dense_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.0005392401074232552}. Best is trial 0 with value: 0.8741999864578247.\n",
            "[I 2025-05-20 14:21:59,544] Trial 1 finished with value: 0.8705999851226807 and parameters: {'embedding_dim': 96, 'lstm_units': 256, 'dense_units': 96, 'dropout_rate': 0.6000000000000001, 'learning_rate': 0.00032045211399809025}. Best is trial 0 with value: 0.8741999864578247.\n",
            "[I 2025-05-20 14:28:53,731] Trial 2 finished with value: 0.8700000047683716 and parameters: {'embedding_dim': 224, 'lstm_units': 192, 'dense_units': 32, 'dropout_rate': 0.3, 'learning_rate': 0.00014607009954030012}. Best is trial 0 with value: 0.8741999864578247.\n",
            "[I 2025-05-20 14:35:47,991] Trial 3 finished with value: 0.8519999980926514 and parameters: {'embedding_dim': 64, 'lstm_units': 224, 'dense_units': 128, 'dropout_rate': 0.7, 'learning_rate': 0.00048557524603788665}. Best is trial 0 with value: 0.8741999864578247.\n",
            "[I 2025-05-20 14:46:46,446] Trial 4 finished with value: 0.8737999796867371 and parameters: {'embedding_dim': 256, 'lstm_units': 224, 'dense_units': 32, 'dropout_rate': 0.3, 'learning_rate': 0.0001635861502195963}. Best is trial 0 with value: 0.8741999864578247.\n",
            "[I 2025-05-20 14:57:46,413] Trial 5 finished with value: 0.8515999913215637 and parameters: {'embedding_dim': 96, 'lstm_units': 256, 'dense_units': 32, 'dropout_rate': 0.7, 'learning_rate': 0.002113000463432984}. Best is trial 0 with value: 0.8741999864578247.\n",
            "[I 2025-05-20 15:04:40,153] Trial 6 finished with value: 0.8550000190734863 and parameters: {'embedding_dim': 256, 'lstm_units': 128, 'dense_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.00018413845473154385}. Best is trial 0 with value: 0.8741999864578247.\n",
            "[I 2025-05-20 15:11:05,231] Trial 7 finished with value: 0.8925999999046326 and parameters: {'embedding_dim': 256, 'lstm_units': 160, 'dense_units': 96, 'dropout_rate': 0.5, 'learning_rate': 0.002105550978190454}. Best is trial 7 with value: 0.8925999999046326.\n",
            "[I 2025-05-20 15:16:46,610] Trial 8 finished with value: 0.8733999729156494 and parameters: {'embedding_dim': 96, 'lstm_units': 96, 'dense_units': 32, 'dropout_rate': 0.3, 'learning_rate': 0.00010402390718638126}. Best is trial 7 with value: 0.8925999999046326.\n",
            "[I 2025-05-20 15:28:40,287] Trial 9 finished with value: 0.8659999966621399 and parameters: {'embedding_dim': 192, 'lstm_units': 224, 'dense_units': 128, 'dropout_rate': 0.5, 'learning_rate': 0.0011185891763868987}. Best is trial 7 with value: 0.8925999999046326.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optuna study complete.\n",
            "Best hyperparameters:  {'embedding_dim': 256, 'lstm_units': 160, 'dense_units': 96, 'dropout_rate': 0.5, 'learning_rate': 0.002105550978190454}\n",
            "Best validation accuracy:  0.8925999999046326\n",
            "\n",
            "Training the final model with optimal hyperparameters.\n",
            "\n",
            "Final model with optimal hyperparameters built and compiled.\n"
          ]
        }
      ]
    }
  ]
}